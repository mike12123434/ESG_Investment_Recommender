"""
ESG Investment Recommendation System
Real web scraping with semantic analysis - No simulated data
"""

import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
from datetime import datetime
import time

warnings.filterwarnings('ignore')

# ========== Configuration ==========

ESG_KEYWORDS = {
    'env': ['綠能', '減碳', '碳排放', '氣候', '再生能源', '淨零', '永續', '環保', '能源轉型', '循環經濟',
            'green', 'carbon', 'climate', 'renewable', 'sustainable', 'esg', 'environmental'],
    'social': ['社會', '員工', '人權', '多元', '平等', '勞工', '福利', '社區', '供應鏈',
               'social', 'labor', 'employee', 'diversity', 'welfare', 'community'],
    'gov': ['治理', '董事會', '透明', '獨立董事', '風險管理', '內控', '合規', '股東',
            'governance', 'board', 'compliance', 'transparency', 'ethics', 'risk']
}

# Fallback ESG ETF list (Taiwan market)
FALLBACK_ETF_LIST = [
    {'code': '00692', 'name': '富邦公司治理'},
    {'code': '00850', 'name': '元大臺灣ESG永續'},
    {'code': '00878', 'name': '國泰永續高股息'},
    {'code': '00888', 'name': '永豐台灣ESG'},
    {'code': '00891', 'name': '中信關鍵半導體'},
    {'code': '00896', 'name': '中信綠能及電動車'},
    {'code': '00899', 'name': '群益ESG投等債20+'},
    {'code': '00900', 'name': '富邦特選高股息30'},
    {'code': '00905', 'name': 'FT臺灣Smart'},
    {'code': '00907', 'name': '永豐優息存股'},
]

# ========== Web Scraping Functions ==========

def scrape_esg_etfs():
    """Scrape ESG ETF list from multiple sources with fallback"""
    
    # Method 1: Try TWSE website
    try:
        url = "https://www.twse.com.tw/zh/esg-index-product/etf"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        response = requests.get(url, headers=headers, timeout=20)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            etfs = []
            
            # Try finding table rows
            for row in soup.find_all('tr'):
                cols = row.find_all(['td', 'th'])
                if len(cols) >= 2:
                    code = cols[0].get_text(strip=True)
                    name = cols[1].get_text(strip=True).split('(')[0].strip()
                    
                    # Validate code format
                    if code and len(code) >= 4 and code[:4].isdigit():
                        etfs.append({'code': code, 'name': name})
            
            if etfs:
                unique_etfs = list({e['code']: e for e in etfs}.values())
                return unique_etfs[:20]
    except Exception as e:
        st.warning(f"TWSE scraping failed: {str(e)}")
    
    # Method 2: Try Yahoo Finance Taiwan ETF list
    try:
        url = "https://tw.stock.yahoo.com/class-quote?sectorId=35&exchange=TAI"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            etfs = []
            
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if '/quote/' in href and '.TW' in href:
                    code = href.split('/')[-1].replace('.TW', '')
                    name = link.get_text(strip=True)
                    
                    if code and len(code) >= 4 and code[:5].isdigit():
                        etfs.append({'code': code, 'name': name})
            
            if etfs:
                unique_etfs = list({e['code']: e for e in etfs}.values())
                return unique_etfs[:15]
    except Exception as e:
        st.warning(f"Yahoo Finance scraping failed: {str(e)}")
    
    # Fallback: Use predefined list
    st.info("Using fallback ETF list")
    return FALLBACK_ETF_LIST


def scrape_yahoo_finance_info(code):
    """Scrape Yahoo Finance Taiwan stock page"""
    try:
        url = f"https://tw.stock.yahoo.com/quote/{code}.TW"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=12)
        response.encoding = 'utf-8'
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract meaningful text from specific sections
            texts = []
            for tag in soup.find_all(['p', 'div', 'span', 'h1', 'h2'], limit=100):
                text = tag.get_text(strip=True)
                if len(text) > 20 and len(text) < 500:
                    texts.append(text)
            
            combined = ' '.join(texts[:30])
            return combined if combined else ""
        return ""
    except Exception:
        return ""


def scrape_moneydj_info(code):
    """Scrape MoneyDJ ETF info"""
    try:
        url = f"https://www.moneydj.com/ETF/X/Basic/Basic0004.xdjhtm?etfid={code}.tw"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=12)
        response.encoding = 'big5'
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            texts = []
            for tag in soup.find_all(['td', 'div', 'p'], limit=80):
                text = tag.get_text(strip=True)
                if len(text) > 15 and len(text) < 300:
                    texts.append(text)
            
            return ' '.join(texts[:25])
        return ""
    except Exception:
        return ""


def scrape_goodinfo_data(code):
    """Scrape company info from Goodinfo"""
    try:
        url = f"https://goodinfo.tw/tw/StockDetail.asp?STOCK_ID={code}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=12)
        response.encoding = 'utf-8'
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            text_content = soup.get_text(separator=' ', strip=True)
            return text_content[:2000] if text_content else ""
        return ""
    except Exception:
        return ""


def scrape_cnyes_info(code):
    """Scrape info from cnyes"""
    try:
        url = f"https://fund.cnyes.com/detail/{code}/profile"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=12)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            texts = []
            
            for tag in soup.find_all(['p', 'div', 'span'], limit=60):
                text = tag.get_text(strip=True)
                if len(text) > 20 and len(text) < 400:
                    texts.append(text)
            
            return ' '.join(texts[:20])
        return ""
    except Exception:
        return ""


def comprehensive_web_scrape(code, name):
    """Scrape multiple sources and combine text"""
    all_text = [name]
    sources_used = []
    
    scrapers = [
        ('Yahoo Finance', lambda: scrape_yahoo_finance_info(code)),
        ('MoneyDJ', lambda: scrape_moneydj_info(code)),
        ('Goodinfo', lambda: scrape_goodinfo_data(code)),
        ('Cnyes', lambda: scrape_cnyes_info(code))
    ]
    
    for source_name, scraper_func in scrapers:
        try:
            text = scraper_func()
            if text and len(text) > 30:
                all_text.append(text)
                sources_used.append(source_name)
            time.sleep(0.4)  # Polite delay
        except Exception:
            continue
    
    combined_text = ' '.join(all_text)
    sources_str = ', '.join(sources_used) if sources_used else 'ETF Name Only'
    
    return combined_text, sources_str


# ========== Semantic Analysis ==========

def analyze_esg_with_tfidf(text):
    """Semantic analysis using TF-IDF and keyword matching"""
    text_lower = text.lower()
    
    # Keyword frequency scoring
    def keyword_score(keywords):
        matches = sum(1 for kw in keywords if kw in text_lower)
        return min(3 + matches * 1.2, 10)
    
    e_base = keyword_score(ESG_KEYWORDS['env'])
    s_base = keyword_score(ESG_KEYWORDS['social'])
    g_base = keyword_score(ESG_KEYWORDS['gov'])
    
    # TF-IDF semantic similarity
    try:
        e_ref = ' '.join(ESG_KEYWORDS['env'])
        s_ref = ' '.join(ESG_KEYWORDS['social'])
        g_ref = ' '.join(ESG_KEYWORDS['gov'])
        
        vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform([text, e_ref, s_ref, g_ref])
        
        similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:4])[0]
        
        e_score = e_base * 0.6 + similarities[0] * 10 * 0.4
        s_score = s_base * 0.6 + similarities[1] * 10 * 0.4
        g_score = g_base * 0.6 + similarities[2] * 10 * 0.4
        
    except Exception:
        e_score, s_score, g_score = e_base, s_base, g_base
    
    return {
        'E': round(max(e_score, 3.0), 1),
        'S': round(max(s_score, 3.0), 1),
        'G': round(max(g_score, 3.0), 1)
    }


# ========== Market Data ==========

def fetch_market_data(code):
    """Fetch market data from Yahoo Finance"""
    try:
        ticker = yf.Ticker(f"{code}.TW")
        hist = ticker.history(period="1y")
        
        if hist.empty or len(hist) < 30:
            # Try without .TW suffix
            ticker = yf.Ticker(code)
            hist = ticker.history(period="1y")
            
        if hist.empty or len(hist) < 30:
            return None
        
        start_price = hist['Close'].iloc[0]
        end_price = hist['Close'].iloc[-1]
        
        if start_price <= 0 or end_price <= 0:
            return None
            
        returns = ((end_price / start_price) - 1) * 100
        volatility = hist['Close'].pct_change().std() * np.sqrt(252) * 100
        
        volatility = max(volatility, 0.1)
        sharpe = ((returns / 100) - 0.02) / (volatility / 100)
        risk_level = int(np.clip(round(volatility / 5), 1, 5))
        
        return {
            'annual_return': round(returns, 2),
            'volatility': round(volatility, 2),
            'risk_level': risk_level,
            'sharpe_ratio': round(sharpe, 2),
            'avg_volume': int(hist['Volume'].mean())
        }
        
    except Exception:
        return None


def process_etf(etf_info, progress_callback=None):
    """Process single ETF with real web scraping"""
    
    if progress_callback:
        progress_callback(f"Fetching market data for {etf_info['code']}...")
    
    market_data = fetch_market_data(etf_info['code'])
    if not market_data:
        return None
    
    if progress_callback:
        progress_callback(f"Scraping web content for {etf_info['code']}...")
    
    scraped_text, sources = comprehensive_web_scrape(etf_info['code'], etf_info['name'])
    
    if not scraped_text or len(scraped_text) < 50:
        scraped_text = etf_info['name'] + " ESG sustainable investment fund"
        sources = "Name only"
    
    esg_scores = analyze_esg_with_tfidf(scraped_text)
    
    return {
        'code': etf_info['code'],
        'name': etf_info['name'],
        **market_data,
        'E_score': esg_scores['E'],
        'S_score': esg_scores['S'],
        'G_score': esg_scores['G'],
        'scraped_text': scraped_text[:1000],
        'full_text_length': len(scraped_text),
        'data_sources': sources,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }


@st.cache_data(ttl=3600, show_spinner=False)
def build_dataset():
    """Build complete dataset with real web scraping"""
    etf_list = scrape_esg_etfs()
    
    if not etf_list:
        return None
    
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total = len(etf_list)
    
    for idx, etf in enumerate(etf_list):
        def update_status(msg):
            status_text.text(f"[{idx+1}/{total}] {msg}")
        
        update_status(f"Processing {etf['code']} - {etf['name']}")
        progress_bar.progress((idx + 1) / total)
        
        result = process_etf(etf, progress_callback=update_status)
        if result:
            results.append(result)
        
        time.sleep(0.3)
    
    progress_bar.empty()
    status_text.empty()
    
    return pd.DataFrame(results) if results else None


def recommend_etfs(df, user_weights, risk_tolerance):
    """Generate recommendations based on user preferences"""
    data = df.copy()
    
    scaler = MinMaxScaler()
    data[['E_norm', 'S_norm', 'G_norm']] = scaler.fit_transform(
        data[['E_score', 'S_score', 'G_score']]
    )
    
    total_weight = user_weights['E'] + user_weights['S'] + user_weights['G']
    user_vec = np.array([[
        user_weights['E'] / total_weight,
        user_weights['S'] / total_weight,
        user_weights['G'] / total_weight
    ]])
    
    data['esg_match'] = cosine_similarity(
        user_vec,
        data[['E_norm', 'S_norm', 'G_norm']]
    )[0]
    
    data['sharpe_norm'] = scaler.fit_transform(data[['sharpe_ratio']])
    data['return_norm'] = scaler.fit_transform(data[['annual_return']])
    
    data['total_score'] = (
        data['esg_match'] * 0.5 +
        data['sharpe_norm'] * 0.3 +
        data['return_norm'] * 0.2
    )
    
    data = data[data['risk_level'] <= risk_tolerance]
    
    if data.empty:
        return None, None
    
    top_5 = data.nlargest(5, 'total_score').copy()
    top_5['allocation'] = (top_5['total_score'] / top_5['total_score'].sum() * 100).round(1)
    
    return top_5, data


# ========== UI ==========

def main():
    st.set_page_config(page_title="ESG Recommender", layout="wide")
    
    st.title("ESG Investment Recommender")
    st.caption("Real-time web scraping with semantic ESG analysis")
    
    with st.sidebar:
        st.subheader("Settings")
        
        risk = st.slider("Risk Tolerance", 1, 5, 3)
        
        st.write("ESG Weights")
        e_weight = st.number_input("Environmental (E)", 0, 100, 40, step=5)
        s_weight = st.number_input("Social (S)", 0, 100, 30, step=5)
        g_weight = st.number_input("Governance (G)", 0, 100, 30, step=5)
        
        total = e_weight + s_weight + g_weight
        if total != 100:
            st.warning(f"Total: {total}%. Should equal 100%")
        
        run_btn = st.button("Start Analysis", type="primary", use_container_width=True)
    
    if run_btn:
        if total != 100:
            st.error("ESG weights must sum to 100%")
            return
        
        with st.spinner("Initializing data collection..."):
            df = build_dataset()
        
        if df is None or df.empty:
            st.error("Unable to collect sufficient data. Please try again later.")
            st.info("This may be due to network restrictions or website access issues.")
            return
        
        st.success(f"Successfully analyzed {len(df)} ETFs")
        
        user_weights = {'E': e_weight, 'S': s_weight, 'G': g_weight}
        top_etfs, full_data = recommend_etfs(df, user_weights, risk)
        
        if top_etfs is None or top_etfs.empty:
            st.warning("No ETFs match your risk tolerance. Try increasing it.")
            return
        
        st.subheader("Top Recommendations")
        
        display_cols = ['name', 'code', 'allocation', 'annual_return', 
                       'sharpe_ratio', 'volatility', 'E_score', 'S_score', 'G_score']
        
        st.dataframe(
            top_etfs[display_cols],
            column_config={
                "allocation": st.column_config.ProgressColumn(
                    "Portfolio %", format="%.1f%%", min_value=0, max_value=100
                ),
                "annual_return": st.column_config.NumberColumn("Return %", format="%.2f%%"),
                "sharpe_ratio": st.column_config.NumberColumn("Sharpe", format="%.2f"),
                "volatility": st.column_config.NumberColumn("Volatility %", format="%.2f%%")
            },
            hide_index=True,
            use_container_width=True
        )
        
        st.subheader("Raw Scraped Data")
        st.caption("All data from real web scraping")
        
        for idx, row in df.iterrows():
            with st.expander(f"{row['name']} ({row['code']}) - {row['data_sources']}"):
                st.write(f"**Data Sources:** {row['data_sources']}")
                st.write(f"**Text Length:** {row['full_text_length']} characters")
                st.write(f"**Scraped at:** {row['timestamp']}")
                st.write(f"**ESG Scores:** E={row['E_score']}, S={row['S_score']}, G={row['G_score']}")
                st.text_area("Scraped Text Preview", row['scraped_text'], height=200, key=f"text_{idx}")
        
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            "Download Complete Dataset (CSV)",
            csv,
            "esg_scraped_data.csv",
            "text/csv",
            use_container_width=True
        )


if __name__ == "__main__":
    main()
